apache-spark
introduction
contexte-général-lexplosion-des-données
limites-des-approches-traditionnelles-sql-et-hadoop
apache-spark-une-réponse-aux-besoins-modernes-du-big-data
présentation-générale-dapache-spark
quest-ce-quapache-spark
philosophie-et-objectifs-de-spark
vitesse
facilité-dutilisation
généralité
concepts-fondamentaux-de-spark
environnement-distribué-le-cluster
partitions
rdd-et-dataframes
transformations-et-actions
dag-directed-acyclic-graph
shuffle
stages
tasks
fonctionnement-général-dapache-spark
ecriture-du-programme
construction-du-dag
découpage-en-stages
création-et-exécution-des-tasks
collecte-des-résultats
architecture-de-spark
le-driver-program
les-executors
le-cluster-manager
principaux-modules-de-spark
spark-sql
spark-streaming
mllib
graphx
méthodologie-de-traitement-avec-apache-spark
traitement-batch-par-lots
principe-général
fonctionnement-avec-spark
cas-dusage-typiques
traitement-streaming-par-flux
principe-général-1
streaming-dans-spark-le-micro-batch
capacités-offertes
cas-dusage-typiques-1
comparaison-batch-vs-streaming
cadre-pratique-et-description-des-données
justification-du-choix-des-données-de-cryptomonnaies
collecte-des-données-connexion-à-la-plateforme-binance
description-des-variables-collectées
métriques-analysées-et-intérêt-du-traitement-distribué
cas-de-traitement-batch
mise-en-place-de-lenvironnement-de-travail
outils-et-logiciels-nécessaires
architecture-générale-et-composants-intégrés
connexion-aux-données-et-lecture-du-dernier-batch-disponible
connexion-aux-données-et-sélection-du-dernier-batch
lecture-des-données-avec-apache-spark
notion-de-batch-et-rôle-de-lhorodatage-run_ts
notion-de-ticker-et-structure-des-données-de-marché
enrichissement-et-construction-des-indicateurs-de-marché
agrégation-des-résultats-et-classements-top-5
génération-automatisée-dun-rapport-html
envoi-automatique-du-rapport-par-courrier-électronique
reproductibilité-du-traitement
configuration-postgresql
configuration-de-lenvoi-de-mails
paramètres-du-batch-spark
exécution-du-batch
cas-de-traitement-streaming
mise-en-place-de-lenvironnement-de-travail-1
installation-et-configuration-dapache-spark-en-local
installation-dapache-spark
spécificités-windows
rôle-de-docker-dans-le-projet
pourquoi-utiliser-docker
docker-desktop-sous-windows
orchestration-des-services-avec-docker-compose
services-déployés
image-docker-et-conteneur-docker
lancement-des-services
kafka-organisation-du-streaming-des-données
notion-de-topic-kafka
topic-utilisé-dans-le-projet
extraction-des-données-en-temps-réel-websocket-binance-vers-kafka
principe-du-websocket
flux-binance-utilisé
envoi-des-données-vers-kafka
traitement-des-flux-avec-spark-structured-streaming
stockage-des-données-traitées-dans-postgresql
dashboard
architecture-et-flux-de-données
rafraîchissement-automatique-et-logique-de-cache
enrichissement-des-données-indicateurs-calculés-côté-dashboard
paramétrage-utilisateur-sidebar
organisation-fonctionnelle-du-dashboard-onglets
overview-résumé-par-actif
graphiques-volume-et-tendances
top-cryptos-classements-et-comparaison
anomalies-alertes-de-marché
détails-crypto-focus-sur-un-actif
apport-du-dashboard-dans-le-pipeline-streaming
forces-et-limites-dapache-spark
forces-dapache-spark
performances-élevées-grâce-au-calcul-en-mémoire
un-moteur-unifié-pour-différents-types-de-traitements
scalabilité-et-parallélisme
résilience-et-tolérance-aux-pannes
limites-dapache-spark
forte-consommation-de-ressources-mémoire
streaming-basé-sur-le-micro-batching
complexité-de-loptimisation-des-performances
moins-adapté-à-la-gestion-dun-très-grand-nombre-de-petits-fichiers
conclusion-générale
ressources-bibliographiques
apache-spark-et-big-data
streaming-de-données-et-kafka
conteneurisation-et-docker
bases-de-données-relationnelles
données-financières-et-api-binance
